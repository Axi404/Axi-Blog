---
title: 'Paper Reading: Embodied AI 2'
publishDate: 2024-12-13
updatedDate: 2025-02-15
description: 从一些 Embodied AI 相关工作中扫过。
heroImage: {src : "thumbnail.webp", color: '#7F2B32' }
category: 'Paper Reading'
tags:
    - 'AI Talk'
    - LLM
    - VLM
    - 'Embodied AI'
---

import { ArxivRating } from 'astro-pure/advanced'

## 前言

Embodied AI 是一个比较新的领域，而且可能横跨的任务也很多，在这方面做的事情来说，可能一些和具身智能具有比较高相关度的 perception 任务，也都会放在其中。

## HPT

<ArxivRating id='2409.20537' tldr='使用 Stem Encoder 和 Decoder 对齐 cross-embodiment 并训练一个 Transformer' rank={4}>

![The pipeline of HPT](https://pic.axi404.top/image.60u8wwy8i3.webp)

HPT 是 Kaiming He 团队在具身领域的新作，可以说是很直接也很本质地解决了 cross-embodiment 任务的问题，也就是使用在多模态领域中一贯使用的 projector 的思想。这个思想属于是看一眼 Pipeline 图就能看懂的，就是对于不同的机器人，使用不同的 stem 把他们投影到同一个空间中，可以理解为一种机器人任务空间，然后在里面进行 transformer，之后再训练不同的 MLP 来投影回 actions。假如在具有无限多数据的情况下，这种方法确实可以简单有效地进行 scaling up 并且很好地迁移到不同的机器人上，不过令我好奇的是，这种方法居然在此之前没有人提出过，在此之前的工作中本栏目提到过一篇 [Extreme Cross-Embodiment](https://arxiv.org/abs/2402.19432)，然而是将不同的模态统一到了动作空间中，类似于无论是移动还是抓取，本质上都是位移以及旋转。HPT 更多还是聚焦在 manipulation 的任务中，直接且本质地给出了这个架构，并且在很多数据上进行了训练。
</ArxivRating>

## RoboDual

<ArxivRating id='2410.08001' tldr='使用 OpenVLA 以及 DP 组成的快慢系统，DP 用 OpenVLA 的输出作为输入' rank={2}>

![](https://pic.axi404.top/image.4jo3v6aymf.webp)

首先在讲解这篇之前，不得不说的是，这篇的作图风格我确实十分的喜欢，可以说无论是 icon 还是配色都十分的好看。那么这一篇 RoboDual 本质上就是使用两个模型的并行运行来代替了单一的模型，用了一个大的 OpenVLA 作为一个 high-level 的模型，之后用 DiT 去做 low-level 的 policy，其中 OpenVLA 的输出作为 conditioned input 给到 DiT 中。说白了其实这种框架本身和 PALM-E 在内的一系列工作都有很大相似之处，也就是 high-level 和 low-level 的设计，但是区别在于从之前的串行改到了并行，一次 high-level 到多次 low-level 的并行，并且因此可以提高帧率，也就增加了控制的细粒度。

![](https://pic.axi404.top/image.4xujm1jr7w.webp)

从 pipeline 中也不难看出思想的核心其实就在于把 OpenVLA 的一个 action 扩成了若干的 action，而且 DiT 负责一种扩写，而 OpenVLA 则负责泛化。这种方法可以说是一种通用的技巧，不同于之前 conditioned input 就是 language，这里的 input 就是 action，所以降低了 DiT 的表征难度并且也让模型跑起来很快，将来的模型想要提速，这种增加一个 stage 的策略可以说是通用的。
</ArxivRating>

## GR-2

<ArxivRating id='2410.06158' tldr='规模更大的 GR-2，展现了泛化能力' rank={3}>

可以说没有什么非常大的 highlight，就是在之前的 GR-1 的基础上的一个拓展性的工作，在更多的数据上进行了预训练以及微调，具体的方法依然不变，还是在大量视频数据里面训练一个 World Model 之后在机器人的数据里面进行动作微调，让模型学会 Action，可以说就是在意料之中。
</ArxivRating>

## Humanoid Manipulation

<ArxivRating id='2410.10803' tldr='对于 DP3 的 scaling up' rank={3}>

这篇文章其实也很直接，说白了就是一篇对于 DP3 的 Scaling up 的论文。里面提出了一种 iDP3，也就是一个 improved 的方法，但是其实就是一些 trick 的集合，在这里也进行一下介绍。第一个就是 camera centric 的 point cloud 输入，这个应该是利好数据预处理的，而且 scaling up 也比较简单；然后就是下采样少一些，这个其实也可以拓展思考一下，其实是不是可以进行上采用呢，用一些模型对点云进行超分；然后就是把视觉编码器的 MLP 变成卷积，这个应该是经验之谈，可以让输出更加平滑，也可以得到更多的编码内容；以及预测更长时间，这个自然会更好。最后从结果来看，Scaling up 的结果很好，皆大欢喜。
</ArxivRating>

## Surfer

<ArxivRating id='2306.11335' tldr='使用 World Model 的 VLA 模型' rank={2}>

![Surfer](https://pic.axi404.top/Surfer.8adam0a9tq.webp)

这篇论文里面主要提出了一个 World Model，可以看出来，本身就是拿了两个现成的 Encoder 把任务以及图像进行了编码，然后预测下一次的 action 以及 frame。不过从 pipeline 来看，我其实确实不是很确定这个 next frame prediction 是否有效果。隔了一个网络然后来传梯度给 Action prediction module，本身是串行的，等于说加了一个模块以增加额外约束，这种方法肯定符合直觉，但是肯定拓展性以及潜力有限，监督信号还是本身加在输出 action 的模型本身会好，这种可以说是一种增加监督的 trick，但是不一定在更大规模的 scaling 中好用，毕竟效率不高，增加了一个开销很大的模型。
</ArxivRating>

## ACT

<ArxivRating id='2304.13705' tldr='CVAE style 的模型，使用 Transformer 作为 encoder，DETR like 的结构作为 decoder' rank={2}>

![ACT](https://pic.axi404.top/image.eskdeuqyp.webp)

这篇论文也是十分经典的论文了，使用了 CVAE 的结构来运行。按照说法，CVAE 也就是 conditional VAE，使用了 VAE，并且使用图像+joint position 作为解码器的输入。由于我没有看过 CVAE，但是大概猜测 conditional 在这个里面其实指的是 image 以及 joint position 在解码器的输入，而至于任务本身，就是重建输入到编码器里面的 action。本身论文里面有很多的细节，包括说在 VAE 编码器的时候只使用 joint position 以及在训练的时候使用 L1 损失而非 L2，在这里就不进行展开了。解码器的结构神似 DETR，包含一个编码器以及一个解码器，不过有必要指出的是，这里其实等于说一共有两个编码器，VAE 本身还有一个。ACT 可以说是比较优雅的 VAE 范式的解决方法，但是不得不说的是，VAE 甚至是传统 diffusion 的策略在当下来看都已经过时了，这种策略可能难以作为一个可以大量 scaling up 的一个策略存在，而且令我疑惑的一个点在于，之前的 VAE 以及 CVAE 因为需要生成图像的 diversity 因此一定需要采样这样一个分布，但是在 manipulation 任务里面，需要一个确定性的 policy 生成，采样一个分布并不重要。而且这样下去全部的信息不久都进入 condition 了吗，那 VAE encoder 的意义何在呢？我不是很理解。
</ArxivRating>

## SceneVerse

<ArxivRating id='2401.09340' tldr='一个 3D Scene 的数据集以及对应的 Caption Model' rank={3}>
![](https://pic.axi404.top/image.6pnlog61i3.webp)

这篇工作做了一个大型的数据集，里面的全部的 scene 应该以扫描出来的为主，然后使用了不同 level 的标注，这是比较有参考价值的。也就是 Scene Level 的标注，比如说“这是一个有床和衣柜的卧室”，以及一个 object level 的标注，比如说“床”，“衣柜”，以及一个 object ref 的标注，也就是物品之间的关系，比如说“床在衣柜的左边”。其里面提出的 Scene Graph 还是很有效地可以用于表示场景中的相对关系的。

![](https://pic.axi404.top/image.2rv87ruraz.webp)

然后提出了一个模型，这个图应该是画的比较好看，但是不太容易看懂。本身包括一个 PCD Encoder，以及一个 Text Encoder，之后使用一个 Transformer 进行编码，建立了四个损失，也都算是 MLLM 的比较经典设计。一个是 ALEBF 的损失，这里面叫 $\mathcal{L}_{obj}$，在输入到 Transformer 之前，使用一个 MLP 来预测 object 的 3D 坐标，然后使用 ALEBF 的损失来优化；然后是 $\mathcal{L}_{obj}$，在输入到 Transformer 之前，使用一个 MLP 来预测 object 的 3D 坐标，然后使用 ALEBF 的损失来优化；然后是 $\mathcal{L}_{obj}$，在输入到 Transformer 之前，使用一个 MLP 来预测 object 的 3D 坐标，然后使用 ALEBF 的损失来优化；然后是 $\mathcal{L}_{obj}$，在输入到 Transformer 之前，对齐 PCD Encoder 以及 Text Encoder 的输出。然后这里面多了一个 Spatial Attention，其实就是把 Object 的空间信息编码了一下，之后才和 Text Encoder 的信息一起输入到 Transformer 中。然后就是一个 $\mathcal{L}_{MLM}$，以及 Transformer 之后把 PCD 和 Text 进行对齐的 $\mathcal{L}_{ref}$。最后还有一个过了 Spatial Attention 的 $\mathcal{L}_{scene}$。可以说各种地方都加了各种的损失。$\mathcal{L}_{obj}$ 是让 PCD Encoder 包含 Object 全部的表征，$\mathcal{L}_{scene}$ 确保 Spatial Attention 的输出和 Scene Level 的标注一致，也就是 Spatial 之后真的编码出了这个场景，$\mathcal{L}_{ref}$ 和 $\mathcal{L}_{MLM}$ 则是保证 Fusion 之后的信息的准确性。中规中矩，十分合理。

</ArxivRating>

## Robot See Robot Do

<ArxivRating id='2409.18121' tldr='生成 Articulated Object 的 3D 模型并生成 Demonstration 的流程' rank={3}>

![](https://pic.axi404.top/image.3yekisxcqv.webp)

不难发现最近的论文有一部分都是这种比较袖珍的 Real2Sim2Real 的流程。简单来说，这篇的方法就是先生成 Mesh，然后进行拆分。根据人工的 Demonstration 中的动作来分析不同的部分的运动方向（比如说看到 Demonstration 中一直让一个盖子绕着某个轴旋转，那么就认为这个盖子是绕着这个轴旋转的），同时通过人工的手的位置来获得双臂机器人的爪子的放置位置，再然后就是常规的求解了，可以直接用例如 cuRobo 求解动作，然后直接在机器人上使用。可以说这种论文看上去都是十分的优雅，给出了一种看上去逻辑自洽的解法，但是实在是并不具备泛化性，而且可以预见的是，其部署也不是十分的方便，仅作为参考还是可以的。

</ArxivRating>

## ReKep

<ArxivRating id='2409.01652' tldr='使用不同点之间的约束来生成 Action 的 Prompt-based 方法' rank={3}>

![](https://pic.axi404.top/image.32i3684dbh.webp)

因为最近要看相关的内容，所以重温了这一篇，而且之前忘记写这一篇了。这一篇的思路其实很直接，或者说大多数的 Prompt-based 方法都很直接，本身就是建立了一系列的点约束。比如说我现在有两个点是在一个盘子的两端，那么我约束这两个点的 z 的差等于 0，也就等价于让这个盘子平放了。使用 VLM 标点，VLM 生成点约束，生成约束下的 Pose，生成约束下的不同 Pose 之间的 Path，通过这四个环节，就可以生成一个动作序列了。本身这篇其实其程度和 CoPa 比较像，属于一个比较具有细粒度而且比较自由的方法，相较于 CoPa 使用向量、面等约束，两个点毕竟也能表示向量，其可实现性还更强，也具有泛化性。看上去还是很不错的，加上其中使用的工具很多（其附录中的内容），代码应该具有参考性。

</ArxivRating>

## OmniManip

<ArxivRating id='2501.03841' tldr='一种集百家之长的 Prompt-based 方法' rank={4}>

![](https://pic.axi404.top/image.2rv9d76fla.webp)

之前已经介绍过了 ViLA, CoPa 以及 ReKep，这一篇可以说是集百家之长。事实上从 Pipeline 的图也就可以看出来了，首先是一个大的 ViLA，包括拆分以及闭环（事实上这并非 ViLA 独有，方便理解这样说罢了），然后是一个 CoPa 的向量的约束生成，并且用这种约束的描述来生成动作，最后是一个 Optimized trajectory 的生成，这部分也和 ReKep 的思路比较像。尽管看上去有点缝合怪，但是还是用到了他们组拿手的 6D Pose Estimator，并且同时还引入了最近比较火的 3D AIGC，可以说是很全面了。在其他领域，为了显示 novelty，大多数工作都会刻意不使用那些有效但是并非基石的方法，而转而使用自己的模型，然而 Robotics 暂时还是需要在有效性以及鲁棒性上进行进一步的探索，具有强大的 Prompt-based 方法，具有强大的数据生成能力，从而具有大量的数据，才能再谈其他，这一篇可以说通过集百家之长，验证了之前种种方法的有效性，并且获得了相当不错的效果。

</ArxivRating>

## DiffuserLite

<ArxivRating id='2401.15443' tldr='Corse to Fine 的 Diffusion 模型' rank={3}>

![](https://pic.axi404.top/image.175ilwwul7.webp)


这一篇的核心思想其实和 Diffusion 关系不大，主要还是 corse to fine 的思想，使用模型生成一个粗粒度的动作预测，然后将其中前两个动作作为下一次预测的首尾，进一步预测。举例来说，比如第一次预测 `1, 8, 64, 512`，第二次可以预测 `1, 2, 4, 8`，将第一次的 `1,8` 作为condition。我认可这种方法可以提升性能，但是对于其可以提升效率这一点表示怀疑，毕竟假如说降噪的步数相同，按理来说这种方法会运行 `n*step` 次才能得到最终结果，而直接预测的话，只需要运行 `step` 次。当然，也可能是因为运算过程的不完全并行导致的，现在运算量低了，效率就提升了。不过在这里至少需要指出一个常见的和 DP 方法比较时候的 trick，即对于 DP，每一次 step 都推理一次，但是实际上并不用这样，DP 在若干步数之内都保持稳健性，所以效率差距可能存在，但是倒也不至于如此悬殊。

</ArxivRating>

