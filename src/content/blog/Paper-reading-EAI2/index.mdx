---
title: 'Paper Reading: Embodied AI 2'
publishDate: 2024-12-13
updatedDate: 2025-04-02
description: 从一些 Embodied AI 相关工作中扫过。
heroImage: {src : "thumbnail.webp", color: '#7F2B32' }
category: 'Paper Reading'
pixivLink: "118143618"
tags:
    - 'AI Talk'
    - LLM
    - VLM
    - 'Embodied AI'
---

import { ArxivRating } from 'astro-pure/advanced'

## 前言

Embodied AI 是一个比较新的领域，而且可能横跨的任务也很多，在这方面做的事情来说，可能一些和具身智能具有比较高相关度的 perception 任务，也都会放在其中。

## HPT

<ArxivRating id='2409.20537' tldr='使用 Stem Encoder 和 Decoder 对齐 cross-embodiment 并训练一个 Transformer' rank={4}>

![The pipeline of HPT](https://pic.axi404.top/image.60u8wwy8i3.webp)

HPT 是 Kaiming He 团队在具身领域的新作，可以说是很直接也很本质地解决了 cross-embodiment 任务的问题，也就是使用在多模态领域中一贯使用的 projector 的思想。这个思想属于是看一眼 Pipeline 图就能看懂的，就是对于不同的机器人，使用不同的 stem 把他们投影到同一个空间中，可以理解为一种机器人任务空间，然后在里面进行 transformer，之后再训练不同的 MLP 来投影回 actions。假如在具有无限多数据的情况下，这种方法确实可以简单有效地进行 scaling up 并且很好地迁移到不同的机器人上，不过令我好奇的是，这种方法居然在此之前没有人提出过，在此之前的工作中本栏目提到过一篇 [Extreme Cross-Embodiment](https://arxiv.org/abs/2402.19432)，然而是将不同的模态统一到了动作空间中，类似于无论是移动还是抓取，本质上都是位移以及旋转。HPT 更多还是聚焦在 manipulation 的任务中，直接且本质地给出了这个架构，并且在很多数据上进行了训练。
</ArxivRating>

## RoboDual

<ArxivRating id='2410.08001' tldr='使用 OpenVLA 以及 DP 组成的快慢系统，DP 用 OpenVLA 的输出作为输入' rank={3}>

![](https://pic.axi404.top/image.4jo3v6aymf.webp)

首先在讲解这篇之前，不得不说的是，这篇的作图风格我确实十分的喜欢，可以说无论是 icon 还是配色都十分的好看。那么这一篇 RoboDual 本质上就是使用两个模型的并行运行来代替了单一的模型，用了一个大的 OpenVLA 作为一个 high-level 的模型，之后用 DiT 去做 low-level 的 policy，其中 OpenVLA 的输出作为 conditioned input 给到 DiT 中。说白了其实这种框架本身和 PALM-E 在内的一系列工作都有很大相似之处，也就是 high-level 和 low-level 的设计，但是区别在于从之前的串行改到了并行，一次 high-level 到多次 low-level 的并行，并且因此可以提高帧率，也就增加了控制的细粒度。

![](https://pic.axi404.top/image.4xujm1jr7w.webp)

从 pipeline 中也不难看出思想的核心其实就在于把 OpenVLA 的一个 action 扩成了若干的 action，而且 DiT 负责一种扩写，而 OpenVLA 则负责泛化。这种方法可以说是一种通用的技巧，不同于之前 conditioned input 就是 language，这里的 input 就是 action，所以降低了 DiT 的表征难度并且也让模型跑起来很快，将来的模型想要提速，这种增加一个 stage 的策略可以说是通用的。
</ArxivRating>

## GR-2

<ArxivRating id='2410.06158' tldr='规模更大的 GR-2，展现了泛化能力' rank={3}>

可以说没有什么非常大的 highlight，就是在之前的 GR-1 的基础上的一个拓展性的工作，在更多的数据上进行了预训练以及微调，具体的方法依然不变，还是在大量视频数据里面训练一个 World Model 之后在机器人的数据里面进行动作微调，让模型学会 Action，可以说就是在意料之中。
</ArxivRating>

## Humanoid Manipulation

<ArxivRating id='2410.10803' tldr='对于 DP3 的 scaling up' rank={3}>

这篇文章其实也很直接，说白了就是一篇对于 DP3 的 Scaling up 的论文。里面提出了一种 iDP3，也就是一个 improved 的方法，但是其实就是一些 trick 的集合，在这里也进行一下介绍。第一个就是 camera centric 的 point cloud 输入，这个应该是利好数据预处理的，而且 scaling up 也比较简单；然后就是下采样少一些，这个其实也可以拓展思考一下，其实是不是可以进行上采用呢，用一些模型对点云进行超分；然后就是把视觉编码器的 MLP 变成卷积，这个应该是经验之谈，可以让输出更加平滑，也可以得到更多的编码内容；以及预测更长时间，这个自然会更好。最后从结果来看，Scaling up 的结果很好，皆大欢喜。
</ArxivRating>

## Surfer

<ArxivRating id='2306.11335' tldr='使用 World Model 的 VLA 模型' rank={2}>

![Surfer](https://pic.axi404.top/Surfer.8adam0a9tq.webp)

这篇论文里面主要提出了一个 World Model，可以看出来，本身就是拿了两个现成的 Encoder 把任务以及图像进行了编码，然后预测下一次的 action 以及 frame。不过从 pipeline 来看，我其实确实不是很确定这个 next frame prediction 是否有效果。隔了一个网络然后来传梯度给 Action prediction module，本身是串行的，等于说加了一个模块以增加额外约束，这种方法肯定符合直觉，但是肯定拓展性以及潜力有限，监督信号还是本身加在输出 action 的模型本身会好，这种可以说是一种增加监督的 trick，但是不一定在更大规模的 scaling 中好用，毕竟效率不高，增加了一个开销很大的模型。
</ArxivRating>

## ACT

<ArxivRating id='2304.13705' tldr='CVAE style 的模型，使用 Transformer 作为 encoder，DETR like 的结构作为 decoder' rank={2}>

![ACT](https://pic.axi404.top/image.eskdeuqyp.webp)

这篇论文也是十分经典的论文了，使用了 CVAE 的结构来运行。按照说法，CVAE 也就是 conditional VAE，使用了 VAE，并且使用图像+joint position 作为解码器的输入。由于我没有看过 CVAE，但是大概猜测 conditional 在这个里面其实指的是 image 以及 joint position 在解码器的输入，而至于任务本身，就是重建输入到编码器里面的 action。本身论文里面有很多的细节，包括说在 VAE 编码器的时候只使用 joint position 以及在训练的时候使用 L1 损失而非 L2，在这里就不进行展开了。解码器的结构神似 DETR，包含一个编码器以及一个解码器，不过有必要指出的是，这里其实等于说一共有两个编码器，VAE 本身还有一个。ACT 可以说是比较优雅的 VAE 范式的解决方法，但是不得不说的是，VAE 甚至是传统 diffusion 的策略在当下来看都已经过时了，这种策略可能难以作为一个可以大量 scaling up 的一个策略存在，而且令我疑惑的一个点在于，之前的 VAE 以及 CVAE 因为需要生成图像的 diversity 因此一定需要采样这样一个分布，但是在 manipulation 任务里面，需要一个确定性的 policy 生成，采样一个分布并不重要。而且这样下去全部的信息不久都进入 condition 了吗，那 VAE encoder 的意义何在呢？我不是很理解。
</ArxivRating>

## SceneVerse

<ArxivRating id='2401.09340' tldr='一个 3D Scene 的数据集以及对应的 Caption Model' rank={3}>
![](https://pic.axi404.top/image.6pnlog61i3.webp)

这篇工作做了一个大型的数据集，里面的全部的 scene 应该以扫描出来的为主，然后使用了不同 level 的标注，这是比较有参考价值的。也就是 Scene Level 的标注，比如说「这是一个有床和衣柜的卧室」，以及一个 object level 的标注，比如说「床」，「衣柜」，以及一个 object ref 的标注，也就是物品之间的关系，比如说「床在衣柜的左边」。其里面提出的 Scene Graph 还是很有效地可以用于表示场景中的相对关系的。

![](https://pic.axi404.top/image.2rv87ruraz.webp)

然后提出了一个模型，这个图应该是画的比较好看，但是不太容易看懂。本身包括一个 PCD Encoder，以及一个 Text Encoder，之后使用一个 Transformer 进行编码，建立了四个损失，也都算是 MLLM 的比较经典设计。一个是 ALEBF 的损失，这里面叫 $\mathcal{L}_{obj}$，在输入到 Transformer 之前，使用一个 MLP 来预测 object 的 3D 坐标，然后使用 ALEBF 的损失来优化；然后是 $\mathcal{L}_{obj}$，在输入到 Transformer 之前，使用一个 MLP 来预测 object 的 3D 坐标，然后使用 ALEBF 的损失来优化；然后是 $\mathcal{L}_{obj}$，在输入到 Transformer 之前，使用一个 MLP 来预测 object 的 3D 坐标，然后使用 ALEBF 的损失来优化；然后是 $\mathcal{L}_{obj}$，在输入到 Transformer 之前，对齐 PCD Encoder 以及 Text Encoder 的输出。然后这里面多了一个 Spatial Attention，其实就是把 Object 的空间信息编码了一下，之后才和 Text Encoder 的信息一起输入到 Transformer 中。然后就是一个 $\mathcal{L}_{MLM}$，以及 Transformer 之后把 PCD 和 Text 进行对齐的 $\mathcal{L}_{ref}$。最后还有一个过了 Spatial Attention 的 $\mathcal{L}_{scene}$。可以说各种地方都加了各种的损失。$\mathcal{L}_{obj}$ 是让 PCD Encoder 包含 Object 全部的表征，$\mathcal{L}_{scene}$ 确保 Spatial Attention 的输出和 Scene Level 的标注一致，也就是 Spatial 之后真的编码出了这个场景，$\mathcal{L}_{ref}$ 和 $\mathcal{L}_{MLM}$ 则是保证 Fusion 之后的信息的准确性。中规中矩，十分合理。

</ArxivRating>

## Robot See Robot Do

<ArxivRating id='2409.18121' tldr='生成 Articulated Object 的 3D 模型并生成 Demonstration 的流程' rank={3}>

![](https://pic.axi404.top/image.3yekisxcqv.webp)

不难发现最近的论文有一部分都是这种比较袖珍的 Real2Sim2Real 的流程。简单来说，这篇的方法就是先生成 Mesh，然后进行拆分。根据人工的 Demonstration 中的动作来分析不同的部分的运动方向（比如说看到 Demonstration 中一直让一个盖子绕着某个轴旋转，那么就认为这个盖子是绕着这个轴旋转的），同时通过人工的手的位置来获得双臂机器人的爪子的放置位置，再然后就是常规的求解了，可以直接用例如 cuRobo 求解动作，然后直接在机器人上使用。可以说这种论文看上去都是十分的优雅，给出了一种看上去逻辑自洽的解法，但是实在是并不具备泛化性，而且可以预见的是，其部署也不是十分的方便，仅作为参考还是可以的。

</ArxivRating>

## ReKep

<ArxivRating id='2409.01652' tldr='使用不同点之间的约束来生成 Action 的 Prompt-based 方法' rank={3}>

![](https://pic.axi404.top/image.32i3684dbh.webp)

因为最近要看相关的内容，所以重温了这一篇，而且之前忘记写这一篇了。这一篇的思路其实很直接，或者说大多数的 Prompt-based 方法都很直接，本身就是建立了一系列的点约束。比如说我现在有两个点是在一个盘子的两端，那么我约束这两个点的 z 的差等于 0，也就等价于让这个盘子平放了。使用 VLM 标点，VLM 生成点约束，生成约束下的 Pose，生成约束下的不同 Pose 之间的 Path，通过这四个环节，就可以生成一个动作序列了。本身这篇其实其程度和 CoPa 比较像，属于一个比较具有细粒度而且比较自由的方法，相较于 CoPa 使用向量、面等约束，两个点毕竟也能表示向量，其可实现性还更强，也具有泛化性。看上去还是很不错的，加上其中使用的工具很多（其附录中的内容），代码应该具有参考性。

</ArxivRating>

## OmniManip

<ArxivRating id='2501.03841' tldr='一种集百家之长的 Prompt-based 方法' rank={4}>

![](https://pic.axi404.top/image.2rv9d76fla.webp)

之前已经介绍过了 ViLA, CoPa 以及 ReKep，这一篇可以说是集百家之长。事实上从 Pipeline 的图也就可以看出来了，首先是一个大的 ViLA，包括拆分以及闭环（事实上这并非 ViLA 独有，方便理解这样说罢了），然后是一个 CoPa 的向量的约束生成，并且用这种约束的描述来生成动作，最后是一个 Optimized trajectory 的生成，这部分也和 ReKep 的思路比较像。尽管看上去有点缝合怪，但是还是用到了他们组拿手的 6D Pose Estimator，并且同时还引入了最近比较火的 3D AIGC，可以说是很全面了。在其他领域，为了显示 novelty，大多数工作都会刻意不使用那些有效但是并非基石的方法，而转而使用自己的模型，然而 Robotics 暂时还是需要在有效性以及鲁棒性上进行进一步的探索，具有强大的 Prompt-based 方法，具有强大的数据生成能力，从而具有大量的数据，才能再谈其他，这一篇可以说通过集百家之长，验证了之前种种方法的有效性，并且获得了相当不错的效果。

</ArxivRating>

## DiffuserLite

<ArxivRating id='2401.15443' tldr='Corse to Fine 的 Diffusion 模型' rank={2}>

![](https://pic.axi404.top/image.175ilwwul7.webp)


这一篇的核心思想其实和 Diffusion 关系不大，主要还是 corse to fine 的思想，使用模型生成一个粗粒度的动作预测，然后将其中前两个动作作为下一次预测的首尾，进一步预测。举例来说，比如第一次预测 `1, 8, 64, 512`，第二次可以预测 `1, 2, 4, 8`，将第一次的 `1,8` 作为condition。我认可这种方法可以提升性能，但是对于其可以提升效率这一点表示怀疑，毕竟假如说降噪的步数相同，按理来说这种方法会运行 `n*step` 次才能得到最终结果，而直接预测的话，只需要运行 `step` 次。当然，也可能是因为运算过程的不完全并行导致的，现在运算量低了，效率就提升了。不过在这里至少需要指出一个常见的和 DP 方法比较时候的 trick，即对于 DP，每一次 step 都推理一次，但是实际上并不用这样，DP 在若干步数之内都保持稳健性，所以效率差距可能存在，但是倒也不至于如此悬殊。

</ArxivRating>

## SOFAR

<ArxivRating id='2502.13143' tldr='训练了 3D 感知模块的 Prompt-based 方法，提出了大量的资产' rank={3}>

![](https://pic.axi404.top/image.6m41683emh.webp)

这一篇论文的工作量确实很大，第一次看到的时候确实被吓到了。首先是提出了一个标注的数据集，叫做 OrienText300K，这个数据集是一个 Objaverse 的数据集，使用 VLM 进行了一些三维标注，然后使用这个数据集训练了一个叫做 PointSO 的方向标注模型（事实上我比较怀疑这个流程和目前已有的一些 6Dof 姿态估计的方法之间的区别），接下来用这个模型进行了一个类似于 CoPA 一样的 Prompt-based 的方法，也就是 SOFAR。感觉是主打的这个 PointSO，因为后面这个方法甚至还在 Navigation 里面使用了，应该是一些这个 PointSO 的应用一样的感觉。

<img src="https://pic.axi404.top/image.1sf6a3qhei.webp" alt="PointSO" style={{width: '50%'}}/>

本质上 PointSO 的效果就是给你一组点云，并且说出来自己想要标注什么（比如说把手），然后 PointSO 会给你标注出这个内容的一个向量。而其结构本身就是比较经典的多模态的一个 Transformer 的结构。

![](https://pic.axi404.top/image.8hglyupay8.webp)

不得不说图画的是十分的好看的，但是这个向量真能如此有效？一想貌似确实差不多。本身 affordance 由 GPT 分析，然后交给 PointSO 进行标注，之后回到 GPT 里面去进一步分析，给出始末位置，之后直接生成动作，感觉是可行的。

同时很吸引人的一点在于，这个模型确实做了大量的实验，本身工作是 Galbot 的，只能说工业界的积累确实足，大量的在 OXE 训练过的模型，确实不一般。

不过还是有几个疑点，首先就是 Objaverse 真的能洗出来 300K 的数据吗，我对此怀疑。同时，这个工作提出了一个 Spatial 的 Bench，这里面用了一些别的模型比如说 RoboPoint 进行对比，这真的是能获得同样的输出吗，还是说都对于这些模型的输出做了一些后处理？

</ArxivRating>

## PIVOT-R

<ArxivRating id='2410.10394' tldr='Surfer 的后续工作，使用异步多模型进行任务拆解、场景预测、动作预测' rank={3}>

![](https://pic.axi404.top/image.361pe6izhr.webp)

首先这篇的画图一眼就似曾相识了，果然是 Surfer 的后续工作。直接进行宏观的找不同吧，一个是引入了异步的机制，本身的三个模块，任务拆解、场景预测、动作预测，都是异步的，可以用不同的速度来运行。可以想到的是任务拆解最慢，场景预测其次，动作预测最快，符合直觉。在这里偏题说一点，对于异步模型来说，其中比较重要的一个学习出来的特点在于，学到了一种时间偏移的迁移性。具体来说对于不同的模型，一个可能依赖于另一个的输出，但是由于二者的 latency 不同，因此一个输出完了之后另一个可能还是上一次的输出，这个时候模型可以直接使用上一次的输出，而不用等待。另一个不同点则在于相比起之前将 action 预测的 token 喂给场景预测，这一次将场景预测的输出喂给了动作预测，可以说是终于符合直觉了，毕竟这是一个动作预测，而不是一个图像生成模型。总的来说合情合理。

</ArxivRating>

## ManipGen

<ArxivRating id='2410.22332' tldr='蒸馏 RL Expert 作为 Policy 并使用 VLM 进行 Long Horizon 的任务执行' rank={2}>

![](https://pic.axi404.top/image.2yyhirfxla.webp)

这篇论文基本上可以说是一图以蔽之。首先在仿真中训练了一堆的 RL Experts，然后都蒸馏到一个模型里面，这里面值得一提的是一个经典的 Sim2Real 方法，就是使用 SegMask/Depth 等作为中间表征。

这种思想的本质上都是认为 Sim2Real 的最大 Gap 在于视觉区别，（其实我之前也有过一个 Idea，直接使用一个风格迁移器，直接把现实场景渲染成 Isaac Sim 或者别的仿真的风格，之后直接作为一个 Plug-and-Play Module 无缝衔接一切 policy，~~有兴趣的可以联系我~~），但是从现在的视角上来看，更加困难的其实是一些更加细节的东西，现实中的很多控制以及噪声的现象很难用仿真去模拟。说白了你用黑布把人的眼睛蒙上，这个人摸到一个绝对光滑的表面或者发现自己的关节万向阻力相同，也会觉得不对劲。

回到正题，之后方法用了 VLM 去拆分任务，之后分步执行。综上，就是意料之中，也不是很现代，说是效果很好，又刷了若干点，但是问题在于，规模貌似又很小，真的有效吗？

讲一句题外话，相较于 RL 最近在 MLLM 大放异彩，但是 EAI 只要依然依赖于仿真，需要大量的物理模拟和渲染，那么大规模 o1-like rl scaling 就很难奏效，因为时间实在是太慢。RL 是一个可以在 Robotics 中快速见效的方法，并且在诸如 Locomotion 等领域应用广泛，但是在环境输入不单调、追求泛化的 Manipulation 与 Navigation 中，短期之内依然看不到曙光。而假如不在这种场合来使用，和 by script 的数据收集比起来，RL 的效率也并不占优，在 Manipulation 中暂时来看只能作为一种短暂的方法而存在。

</ArxivRating>

## DemoGen

<ArxivRating id='2502.16932' tldr='使用 Replay 进行数据生成' rank={2}>

![](https://pic.axi404.top/image.1e8s3acylt.webp)

也算是一篇比较直观的论文，Object-centric 的思路进行数据生成，检测物体的位置，按照之前采集的轨迹变换到新的轨迹，然后 replay，进行数据生成。然而我认为这种 position 泛化的 scaling 本身已经是比较不是特别有效的 scaling 了，可能物体以及其他环境的 diversity 更重要，因此这篇可能将来收集 demo 场景会比较有用。不过有没有可能，demogen 的 demo 不是 demonstration 而是 demo 呢？

</ArxivRating>

## ArticuBot

<ArxivRating id='2503.03045' tldr='使用 articulation object 的 axis 进行数据生成' rank={3}>

本身论文里没有配很直观的图。论文提了一个方法和一个数据生成流程，方法没什么意思，主要将数据生成的流程。Articulation object，比如说笔记本或者柜子其实是大多数的，这些物体都只有一个自由度，因此假如说你标注了一个可以抓握的 pose，你就可以用这个轴进行旋转，并且通过变换得到旋转后的 Pose，进而组成 action 序列，然后生成数据。这是一个简单有效的方法，说实话我之前一月份的时候也提出过这个 idea，但是目前他们做出来了。抓住绝大多数 articulation 物体只有一个自由度，然后对这个自由度施加变换是一个很本质的事情，并且可以借此直接解决绝大多数 articulation 数据生成的问题。

</ArxivRating>

## LAPA

<ArxivRating id='2503.14734' tldr='可以用 Video 数据来进行预训练' rank={4}>

![](https://pic.axi404.top/image.32i50qt7z0.webp)

这一篇作为 GR00T 的前置来看，分为了三个步骤，分别是 action 的 pretrain，VLA 的 pretrain 以及一个 finetune。本身最有意思的还是第一个环节，也就是用 VQ-VAE 来生成 latent action 的这个操作。可以看到 $x_1$ 和 $x_2$ 是相隔了几帧的图片，输入到 encoder 里面求出来 z，之后用 $z$ 和 $x_1$ 恢复 $x_2$。这个流程有趣的地方是，我们可以获得这样一个比喻，本质上 encoder 做的事情就是求逆运动学，也就是已知当前和目标，求中间的动作；而 decoder 做的事情就是求正运动学，也就是已知当前和动作，求目标。这个流程之后，我们就已经给全部的 Video 数据凭空标注了 action 信息了，这种 action 称之为 latent action。需要注意的是，latent action 并不是某种类似于 action 经过 embedding 之后转化为了 latent，而是可以理解为描述了某种抽象的 embodiment 的 action。即，latent action 和诸如 franka action 等是一个 level 的。之后就是第二阶段，本身可以理解为在用 Video 的 V 以及 GPT 标注的 L 以及这里获得的 A 来将 VLM 预训练为 VLA，之后最后第三阶段，在真实的数据集上去 finetune。

</ArxivRating>

## GR00T

<ArxivRating id='2503.14734' tldr='解决了数据金字塔的端到端 VLA' rank={4}>

![](https://pic.axi404.top/image.7zqluepb7d.webp)

这一篇是我非常喜欢的了。本身的话，虽然你看这个 pipeline 很像是某种双系统，而且论文中也可以看到类似频率差异的表述，但是实际上只是一个快慢系统罢了。其中 VLM 只是一个 VL encoder，而下面的部分则是 action encoder，最后放到一起 fusion。很有趣的一点是，这篇使用了 LAPA 的方法来做 Web Video 数据的标注，从而将数据金字塔，即 `Real-World Data + Synthetic Data + Web Data & Human Videos` 都统一为了相同的 VLA 格式。之后就是直接预训练了，没啥问题。

</ArxivRating>
