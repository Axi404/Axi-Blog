---
title: Manipulation RL 的终结
publishDate: 2025-03-11
updatedDate: 2025-03-11
description: 一则非内行人士的不专业看法。
heroImage: {src : "thumbnail.webp", color: '#13378D' }
category: 'Tech Talk'
pixivLink: '127766591'
draft: true
tags:
    - 'Tech Talk'
    - 'RL'
---

## 前言

伴随着 Deepseek R1 的横空出世，GRPO 为主的 RL 方法在 MLLM 中进行 fine-tuning 的浪潮迎来了高潮，而在此之前，伴随着 o1/o3 的发布，学界在此话题上的也早已经探索了许久。然而事实上，RL 这一历史悠久的方法，最早在 rule-based 的游戏中被长足发展，诸如 AlphaGo 等，均在历史上留下了浓墨重彩的一笔。

然而，在 Robotics 这一领域，RL 方法的探索似乎并不如在 MLLM 中那么顺利。事实上 RL 是一种见效极快的方法，在大多数的任务中，依赖于 GPU 的高度并行，同时训练大量的 RL-policy，大概率都可以得到一个不错的结果，在 locomotion 任务中，更是如此。然而，尽管当前也有一些基于 RL 的 Manipulation 任务的方法，而且看上去效果也不错，但是，以鄙人之愚见，RL 在 Manipulation 任务中似乎即将迎来终结。

## 定义

首先让我们区分 RL 和 RLHF。事实上，在我看来，MLLM 的大多数 RL，其本质上依然是 RLHF，即按照某种偏好进行调优。RL 与 RLHF 在我的观点中，其本质的区别在于，目标是完成任务还是满足人的偏好。诸如，对于一个 Manipulation policy，把香蕉放到盘子里，这是一个完成任务的例子；而平稳丝滑地把香蕉放到盘子里，gripper 不会过度地晃动，则是一个满足人的偏好的例子。通过 RLHF 的方法，将一些更合理的抓握方法来 fine-tuning 一个 policy，来显式引导模型做出合理的抓握，这听上去是一个合理的过程，会让过程变得优雅，但是也可能陷入之前 MLLM 所谓「难以超过人类的困境」，不过目前来看，这种方法确实还有巨大的研究空间。

然而标准的 RL 却并非如此。

## 生态位的窘境

### 我们的目标

事实上，我们需要认清的是，无论是从 LLM 出发，还是从具身智能的角度来看，本质上我们都造出了一个模型。这些模型有着一个惊人的相同点，他们的输入是一个目标以及感知信息，模型利用某种接口与现实或者虚拟世界进行交互，以达成这个目标。

为了让 LLM 可以操控电脑，出现了诸如 computer use 在内的 agent 技术路线，让各种 API 作为一种接口。LLM 的输出是以文本形式的（同时输出文本和图像的范式在之后也会流行，或者说一些 Multi-In Multi-Out 的范式），因此我们把诸多的接口统一成了文本形式，以供模型调用，而假如有了全世界所有的接口，有了一个万能的输出文本的模型，并且全部的接口都可以用文本的形式来表述，那么 AGI 就降临了。

具身智能领域同样在寻找这样一种 AGI，而不同于 LLM 的文本，这里的模型输出动作，这是一种更加物理的表征，可以直接和现实世界交互，甚至可以用一种极为简单的方式来替代 LLM 所做的全部事情，也就是敲键盘（虽然这个任务短期来看是天方夜谭），物理天然是一个万能的接口，至少对于人类来说，我们假设 AGI 的初级目标是逼近人类，那么这个接口已经足够万能了。因此，相较于 LLM，具身智能的范式具有一种天然的优势，它只需要泛化就好了，也就是一个「万能的手」。

既然已经确定了目标，那么让我们谈一谈如何实现泛化。我会单独再写一篇文章，来讨论一下端到端和双系统的优劣，以及为什么当下我为什么更加看好双系统，不过一个显然的事情是，端到端是远比双系统更困难的事情，于是我们来分析这个简单的情况下我们需要实现什么。

按照我们现在对于智能模型的理解，对于一个任务，模型需要学会推理理解，对于当前的空间具有充分的认识，规划自己的待办事项，有序的完成任务，并且具有纠错功能，这其中大多数都像是 LLM 需要实现的内容，因此按照双系统的理解，我们将之前提到的万能的手，可以再次拆分成一个万能的脑和一个万能的手，这听上去很耳熟不是吗？这像极了我们刚才说到的 tool-use agent，一个 LLM 输出语言格式的指令并且调用 API，这个 API，究其根本就是我们万能的手的模型，并且输出动作。

假设长时序任务已经可以被 LLM 有效地拆分成短时序任务，那么对于所谓具身智能的模型所需要完成的事情，就是训练出来一个可以执行短时序任务的模型了。

### 泛化

训练模型是深度学习领域一个最为常见的话题，而如何训练模型，尤其是训练一个可以泛化的模型，目前只有一条路线被广泛使用，那就是预训练。

我们一直以来都对于从预训练后的模型中压榨潜能具有很多的心得，无论是直接 Fine-tuning，还是使用诸如 LoRa 的技巧，又或者是 RLHF 或者 GRPO 的方法，这些都是可以的，并且在这种训练过程中，也可以让模型获得更多的性能以及更多的知识，而在此之前那个最初的模型，依然要从广大的数据中获取表征。

此时，我们需要谈论起 RL 的关键弊病：效率。一个泛化的模型至少需要多达 10M 量级以上的数据才可以得到，而众所周知，绝大多数的 RL 场景都是人为搭建并且设置目标，不过好在这部分还可以通过一些脚本来实现，但是对于训练数据的获取速度，则是一个无法忽略的问题。

尽管当下的物理仿真引擎已经可以以惊人的速度进行物理仿真，然而每在引擎中 step 一次，所达成的性能开销依然是直接读取数据所难以比较的，而这甚至是在我们认为直接 RL 是可以获得泛化的这个弱假设的基础之上。假如这个假设不成立呢？那么对于实现泛化的模型来说，RL 仅仅剩下了最后一条路，也就是使用仿真生成数据。

## 生成数据

众所周知的两种方法可以用来生成数据，一种是 by script 的方法，而另一种就是通过 rl 的方法。By script 显然是更快的，然而伴随着众多工作的推出，无论是简单的 pick and place 任务，还是更加困难的 articulation 操作的任务，都已经确信可以使用 script 去解决了，不同于仿真生成数据早期的诸多范式，需要完全的 by script 去写清楚抓取的物体的位置以及详细的目标区域，目前的方法可以通过简单的 `{obj1: banana, relation: top, obj2: plate}` 的方式来生成数据。By script 的方法可以用相较于 rl 相比更快的速度来生成数据。